{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT4080 Data Programming with Python (online) - Project k nearest neighbours on the TunedIT data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as npr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the project we will study the method of k nearest neighbours applied to a  music classification data set.These data come from the TunedIT website  http://tunedit.org/challenge/music-retrieval/genres Each row corresponds to a different sample of music from a certain genre. The original challenge was to classify the different genres (the original  prize for this was hard cash!). However we will just focus on a sample of the data (~4000 samples) which is either rock or not. There are 191  characteristics (go back to the website if you want to read about these) The general tasks of this exercise are to: \n",
    "- Load the data set - Standardise all the columns \n",
    "- Divide the data set up into a training and test set \n",
    "- Write a function which runs k nearest neighbours (kNN) on the data set.   (Don't worry you don't need to know anything about kNN) \n",
    "- Check which value of k produces the smallest misclassification rate on the    training set \n",
    "- Predict on the test set and see how it does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 \n",
    "### Load in the data using the pandas read_csv function. The last variable  'RockOrNot' determines whether the music genre for that sample is rock or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/joshward/Desktop/Fin. Math/Y4S1/Data Programming/'\n",
    "\n",
    "house_votes = pd.read_csv(path+'house_votes.csv')\n",
    "tunedit_genres = pd.read_csv(path+'tunedit_genres.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What percentage of the songs in this data set are rock songs (to 1 d.p.)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ans: 48.8 %\n"
     ]
    }
   ],
   "source": [
    "#since we know it is the last variable as mentioned above we can reference it as [-1]\n",
    "#values in the column are integers so must be set to integers as they are binary (0,1)\n",
    "\n",
    "tunedit_genres.iloc[:,-1] = tunedit_genres.iloc[:,-1].astype(int)\n",
    "percent = round(100*tunedit_genres.iloc[:,-1].sum()/len(tunedit_genres),1)\n",
    "print(\"Ans:\",percent,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 \n",
    "### To perform a classification algorithm, you need to define a classification  variable and separate it from the other variables. We will use 'RockOrNot' as  our classification variable. Write a piece of code to separate the data into a  DataFrames X and a Series y, where X contains a standardised version of  everything except for the classification variable ('RockOrNot'), and y contains  only the classification variable. To standardise the variables in X, you need to subtract the mean and divide by the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate(df,class_column):\n",
    "    X = df.drop(columns=[class_column])\n",
    "    y = df.loc[:,class_column]\n",
    "    X = X.astype(float)\n",
    "    X = (X - X.mean(axis=0)) / X.std() #standardise variables in X\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = separate(tunedit_genres,'RockOrNot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 \n",
    "### Which variable in X has the largest correlation with y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ans: PAR_SFM_M\n"
     ]
    }
   ],
   "source": [
    "#use abs because it asks for largest which can be taken as positive or negative\n",
    "\n",
    "print(\"Ans:\",abs(X.corrwith(y,axis=0)).idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 \n",
    "### When performing a classification problem, you fit the model to a portion of  your data, and use the remaining data to determine how good the model fit was. \n",
    "### Write a piece of code to divide X and y into training and test sets, use 75% of the data for training and keep 25% for testing. The data should be randomly selected, hence, you cannot simply take the first, say, 3000 rows. If you select  rows 1,4,7,8,13,... of X for your training set, you must also select rows  1,4,7,8,13,... of y for training set. \n",
    "### Additionally, the data in the training set cannot appear in the test set, and vice versa, so that when recombined, all data is accounted for. \n",
    "### Use the seed 123 when generating random numbers Note: The data may not spilt equally into 75% and 25% portions. In this  situation you should round to the nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(seed,X,y,percent_train):\n",
    "    X_training = X.sample(frac=percent_train,random_state=seed)\n",
    "    y_training = y.sample(frac=percent_train,random_state=seed)\n",
    "    \n",
    "    X_test = X.drop(X_training.index,inplace=False)\n",
    "    y_test = y.drop(y_training.index,inplace=False)\n",
    "    \n",
    "    #We must reindex in order to make the data frames easily accessible\n",
    "    X_training = X_training.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_training = y_training.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    return X_training,X_test,y_training,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training,X_test,y_training,y_test = divide(123,X,y,0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 \n",
    "### What is the percentage of rock songs in the training dataset and in the test dataset? Are they the same as the value found in Q1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of rock songs in the training dataset is 49.4 %\n",
      "The percentage of rock songs in the test dataset is 47.1 %\n",
      "They are not the same as the value found in Q1 but are both within 2% of the value from Q1 of 48.8 %\n"
     ]
    }
   ],
   "source": [
    "#sum the column and divide by the column length to get proportion\n",
    "train_percent = round(100*(y_training.sum()/len(y_training)),1)\n",
    "test_percent = round(100*(y_test.sum()/len(y_test)),1)\n",
    "\n",
    "#I have taken these to be rounded to 1.d.p since we are comparing with Q1 which was asked to be given to 1.d.p\n",
    "\n",
    "print(\"The percentage of rock songs in the training dataset is\", train_percent,'%')\n",
    "print(\"The percentage of rock songs in the test dataset is\", test_percent,'%')\n",
    "print(\"They are not the same as the value found in Q1 but are both within 2% of the value from Q1 of\",percent,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 \n",
    "### Now we're going to write a function to run kNN on the data sets. kNN works By the following algorithm:\n",
    "1. Choose a value of k (usually odd)\n",
    "2. For each observation, find its k closest neighbours\n",
    "3. Take the majority vote (mean) of these neighbours\n",
    "4. Classify observation based on majority vote\n",
    "\n",
    "### We're going to use standard Euclidean distance to find the distance between observations, defined as sqrt( (xi - xj)^T (xi-xj) ) A useful short cut for this is the scipy functions pdist and squareform\n",
    "\n",
    "### The function inputs are:\n",
    "1. DataFrame X of explanatory variables \n",
    "2. binary Series y of classification values \n",
    "3. value of k (you can assume this is always an odd number)\n",
    "\n",
    "### The function should produce:\n",
    "1. Series y_star of predicted classification values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K nearest neighbours algorithm is non-parametric as it identifies k nearest data points to a classification value\n",
    "#Once it has done this it notes from which explanatory variable\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def kNN(X,#explanatory variables\n",
    "        y,#binary series of classification values\n",
    "        k):#can assume this is always an odd number\n",
    "    \n",
    "    n = len(y) #Find the number of obsvervation\n",
    "    \n",
    "    y_star = np.zeros(len(y)) #Set up return values\n",
    "    \n",
    "    #Calculate the distance matrix for the observations in X\n",
    "    dist = squareform(pdist(X,'euclidean'),force='tomatrix')\n",
    "    \n",
    "    #Make all the diagonals very large so it can't choose itself as a closest neighbour\n",
    "    dist[np.diag_indices_from(dist)] = 1e10 #not necessary when using argpartition but did it anyway\n",
    "\n",
    "    y_star = np.zeros(len(X))\n",
    "    dist = np.argpartition(dist,k,axis=1)\n",
    "    \n",
    "    #Loop through each observation to create predictions\n",
    "    for i in range(len(dist)):\n",
    "        #Find the y values of the k nearest neighbours. Now allocate to y_star\n",
    "        y_star[i] = round(y[dist[i,:k]].sum()/k)\n",
    "        \n",
    "    y_star = pd.Series(y_star,index=[y.index[i] for i in range(len(y))]) #convert y_star into series\n",
    "    return y_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 \n",
    "### The misclassification rate is the percentage of times the output of a  classifier doesn't match the classification value. Calculate the  misclassification rate of the kNN classifier for X_train and y_train, with k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclass(X,y,k):\n",
    "    misclassification = 0\n",
    "    KNN = kNN(X,y,k)\n",
    "    \n",
    "    for i in range(len(KNN)):\n",
    "        if KNN[i] != y[i]:\n",
    "            misclassification += 1\n",
    "\n",
    "    return 100*(misclassification/len(KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The misclassification rate for k=3 is 4.701567189063021%\n"
     ]
    }
   ],
   "source": [
    "print(\"The misclassification rate for k=3 is {}%\".format(misclass(X_training,y_training,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8 \n",
    "### The best choice for k depends on the data. Write a function kNN_select that  will run a kNN classification for a range of k values, and compute the  misclassification rate for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function should produce:\n",
    "# - a Series mis_class_rates, indexed by k, with the misclassification rates for \n",
    "# each k value in k_vals\n",
    "\n",
    "def kNN_select(X, #- DataFrame X of explanatory variables \n",
    "               y, #- DataFrame X of explanatory variables\n",
    "               k_vals): # - a list of k values k_vals\n",
    "    mis_class_rates = pd.Series(np.nan,index=k_vals)\n",
    "    for i in k_vals:\n",
    "        mis_class_rates[i]=misclass(X,y,i) #this runs a kNN classification within it as instructed\n",
    "                            \n",
    "    return mis_class_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9\n",
    "### Run the function kNN_select on the training data for k = [1, 3, 5, 7, 9]  and find the value of k with the best misclassification rate. Use the best  value of k to report the mis-classification rate for the test data. What is  the misclassification percentage with this k on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3.334445\n",
       "3    4.701567\n",
       "5    5.201734\n",
       "7    5.968656\n",
       "9    6.402134\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_vals = [1,3,5,7,9] #odd numbers to ensure no tied values\n",
    "\n",
    "kNN_select(X_training,y_training,k_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of k from training data is 1 which gave a misclassification percent of 3.334444814938313% for the training set\n"
     ]
    }
   ],
   "source": [
    "print(\"The best value of k from training data is\",kNN_select(X_training,y_training,k_vals).idxmin(), 'which gave a misclassification percent of {}%'.format(min(kNN_select(X_training,y_training,k_vals))),'for the training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ans: 5.0% on the test data which occurs for k= 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Ans: {}%\".format(min(kNN_select(X_test,y_test,k_vals))),\"on the test data which occurs for k=\",\n",
    "      kNN_select(X_test,y_test,k_vals).idxmin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10\n",
    "### Write a function to generalise the k nearest neighbours classification  algorithm. The function should:\n",
    "- Separate out the classification variable for the other variables in the dataset,   i.e. create X and y.\n",
    "- Divide X and y into training and test set, where the number in each is    specified by 'percent_train'.\n",
    "- Run the k nearest neighbours classification on the training data, for a set    of k values, computing the mis-classification rate for each k\n",
    "- Find the k that gives the lowest mis-classification rate for the training data,   and hence, the classification with the best fit to the data.\n",
    "- Use the best k value to run the k nearest neighbours classification on the test   data, and calculate the mis-classification rate\n",
    "### The function should return the mis-classification rate for a k nearest neighbours classification on the test data, using the best k value for the training data You can call the functions from Q6 and Q8 inside this function, provided they  generalise, i.e. will work for any dataset, not just the TunedIT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_classification(df,class_column,seed,percent_train,k_vals):\n",
    "    # df            - DataFrame to \n",
    "    # class_column  - column of df to be used as classification variable, should\n",
    "    #                 specified as a string  \n",
    "    # seed          - seed value for creating the training/test sets\n",
    "    # percent_train - percentage of data to be used as training data\n",
    "    # k_vals        - set of k values to be tests for best classification\n",
    "    \n",
    "    # Separate X and y\n",
    "    X,y = separate(df,class_column)\n",
    "    \n",
    "    # Divide into training and test\n",
    "    X_training,X_test,y_training,y_test = divide(seed,X,y,percent_train)\n",
    "    \n",
    "    # Compute the mis-classification rates for each for the values in k_vals\n",
    "    missclass = kNN_select(X_training,y_training,k_vals)\n",
    "        \n",
    "    # Find the best k value, by finding the minimum entry of mis_class_rates \n",
    "    k = missclass.idxmin()\n",
    "    \n",
    "    # Run the classification on the test set to see how well the 'best fit'\n",
    "    kNN(X_test,y_test,k)\n",
    "    \n",
    "    # classifier does on new data generated from the same source\n",
    "    \n",
    "    \n",
    "    # Calculate the mis-classification rates for the test data\n",
    "    mis_class_test = misclass(X_test,y_test,k)\n",
    "        \n",
    "    return mis_class_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your function with the TunedIT data set, with class_column = 'RockOrNot', seed = the value from Q4, percent_train = 0.75, and k_vals = set of k values from Q8, and confirm that it gives the same answer as Q9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kNN_classification(df,class_column,seed,percent_train,k_vals)\n",
    "\n",
    "k_vals = [1,3,5,7,9] #odd numbers to ensure no tied values\n",
    "\n",
    "kNN_classification(tunedit_genres,'RockOrNot',123,0.75,k_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the same as our answer from Q9 which was;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ans: 5.0% which occurs for k= 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Ans: {}%\".format(min(kNN_select(X_test,y_test,k_vals))),\"which occurs for k=\",\n",
    "      kNN_select(X_test,y_test,k_vals).idxmin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now test your function with another dataset, to ensure that your code  generalises. You can use the house_votes.csv dataset, with 'Party' as the  classifier. Select the other parameters as you wish. This dataset contains the voting records of 435 congressman and women in the  US House of Representatives. The parties are specified as 1 for democrat and 0 for republican, and the votes are labelled as 1 for yes, -1 for no and 0 for abstained. Your kNN classifier should return a mis-classification for the test data (with  the best fit k value) of ~8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.256880733944955"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_vals = [1,3,5,7,9] #odd numbers to ensure no tied values\n",
    "\n",
    "kNN_classification(house_votes,'Party',123,0.75,k_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As we can see this provides a misclassification rate of 8.256880733944955% which is roughly ~8% as predicted\n"
     ]
    }
   ],
   "source": [
    "print('As we can see this provides a misclassification rate of {}%'.format(kNN_classification(house_votes,'Party',123,0.75,k_vals)),'which is roughly ~8% as predicted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
